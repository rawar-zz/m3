{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run this code you will need to download the 'imagenet-vgg-verydeep-19.mat' file and put it to the same folder with this file.\n",
    "You can download it from here: \n",
    "https://dl.dropboxusercontent.com/u/108721752/imagenet-vgg-verydeep-19.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_file_name = '1-content.jpg'\n",
    "style_file_name = '1-style.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('./imagenet-vgg-verydeep-19.mat')\n",
    "\n",
    "image_content = scipy.misc.imread(content_file_name)\n",
    "#image_content= scipy.misc.imresize(image_content,(224,224))\n",
    "plt.figure(figsize=(11,11))\n",
    "plt.imshow(image_content)\n",
    "image_content = image_content.astype('float32')\n",
    "image_content = np.ndarray.reshape(image_content,((1,) + image_content.shape)) # 1 means batch_size\n",
    "#image_content = np.ndarray.reshape(image_content.shape + (1,)) it is needed to handle gray scale image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_style = scipy.misc.imread(style_file_name)\n",
    "#image_style= scipy.misc.imresize(image_style,(224,224))\n",
    "plt.figure(figsize=(11,11))\n",
    "plt.imshow(image_style)\n",
    "image_style = image_style.astype('float32')\n",
    "image_style = np.ndarray.reshape(image_style,((1,) + image_style.shape)) # 1 means batch_size\n",
    "#image_style = np.ndarray.reshape(image_style.shape + (1,)) it is needed to handle gray scale image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_content.shape)\n",
    "print(image_style.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
    "            padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "def _pool_layer(input):\n",
    "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "            padding='SAME')\n",
    "\n",
    "def preprocess(image, mean_pixel):\n",
    "    return (image - mean_pixel).astype('float32')\n",
    "\n",
    "def unprocess(image, mean_pixel):\n",
    "    return (image + mean_pixel).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a network with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(input_image):\n",
    "    layers = (\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "\n",
    "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "        'relu5_3', 'conv5_4', 'relu5_4'\n",
    "    )\n",
    "\n",
    "    weights = data['layers'][0]\n",
    "    \n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(layers):\n",
    "        print(i,name)\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i][0][0][0][0]\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "         \n",
    "\n",
    "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "            #print('kernel:', kernels.shape)\n",
    "            #print('bias:', bias.shape)\n",
    "            \n",
    "            bias = bias.reshape(-1)\n",
    "            current = _conv_layer(current, kernels, bias)\n",
    "            #print('current:', current)\n",
    "            \n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "            #print('current:', current)\n",
    "        elif kind == 'pool':\n",
    "            current = _pool_layer(current)\n",
    "            #print('current:', current)\n",
    "        net[name] = current\n",
    "        #print(' ')\n",
    "\n",
    "    assert len(net) == len(layers)\n",
    "    return net#, mean_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the same network with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_model(input_image):\n",
    "    layers = (\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "\n",
    "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "        'relu5_3', 'conv5_4', 'relu5_4'\n",
    "    )\n",
    "\n",
    "    weights = data['layers'][0]\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    current = input_image\n",
    "    for i, name in enumerate(layers):\n",
    "        print(i,name)\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i][0][0][0][0]\n",
    "            print(kernels.shape, bias.shape)\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "                 \n",
    "            #kernels_keras = np.transpose(kernels, (3,2,1, 0))\n",
    "            kernels_keras = np.transpose(kernels, (1,0,2,3))\n",
    "           \n",
    "            if i==0:\n",
    "                model.add(ZeroPadding2D((1,1),data_format=\"channels_first\",input_shape=(input_image.shape[3],input_image.shape[1],input_image.shape[2])))\n",
    "            else:\n",
    "                model.add(ZeroPadding2D((1,1),data_format=\"channels_first\"))\n",
    "            \n",
    "            print(model.layers[i].input)\n",
    "            print(model.layers[i].output)\n",
    "            model.add(Conv2D(bias.size, (3, 3), name=name, data_format=\"channels_first\", weights=[kernels_keras,bias[0]])) \n",
    "            \n",
    "            #model.add(Conv2D)\n",
    "            #keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, \n",
    "            #                    dilation_rate=(1, 1), \n",
    "            #                    activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n",
    "            #                    bias_initializer='zeros',\n",
    "            #                    kernel_regularizer=None, \n",
    "            #                    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "            #                    bias_constraint=None)\n",
    "        elif kind == 'relu':\n",
    "            model.add(Activation('relu', name=name))\n",
    "        elif kind == 'pool':\n",
    "            model.add(MaxPooling2D((2,2), data_format=\"channels_first\", name=name, border_mode='same', strides=(2,2)))\n",
    "             \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the mean pixel values of the VGG network. We will use these values for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normalization'][0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = data['normalization'][0][0][0]\n",
    "mean_pixel = np.mean(mean, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us preprocess the content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_pre = preprocess(image_content, mean_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the activities of the VGG net on the content_pre image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_keras_model = keras_model(content_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = net(content_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(content_keras_model, show_shapes=True, to_file='model.png')\n",
    "#Image(\"model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the layers that will be used to get the \"content features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONTENT_LAYER = 'relu4_2'\n",
    "# content_features = {}\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     content_pre = preprocess(image_content, mean_pixel)\n",
    "#     content_net = net(np.squeeze(data['layers']), content_pre)\n",
    "#     content_features[CONTENT_LAYER] = content_net[CONTENT_LAYER].eval()\n",
    "\n",
    "CONTENT_LAYERS = ('conv1_1', 'conv2_1', 'conv4_1', 'conv4_2')\n",
    "#CONTENT_LAYERS = ('relu4_2',)\n",
    "content_features = {}\n",
    "content_keras_features ={}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in CONTENT_LAYERS:\n",
    "    # create a temporal subnetwork up to the actual layer \n",
    "    tmpmodel=Model(input=content_keras_model.input, output=content_keras_model.get_layer(layer).output) \n",
    "    \n",
    "    # calculate the values of the \"content features\" features on the content image\n",
    "    content_keras_features[layer]=np.transpose(tmpmodel.predict(np.transpose(content_pre,(0,3,1,2)) ),(0,2,3,1))\n",
    "    print('content_keras_feaures shape:', layer, content_keras_features[layer].shape)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new image we create, we want to match these content features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_features=content_keras_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the layers that will be used to get the \"style features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STYLE_LAYERS = ('conv3_1','conv5_1')\n",
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
    "\n",
    "style_pre = preprocess(image_style, mean_pixel)\n",
    "style_keras_model = keras_model(style_pre)\n",
    "\n",
    "style_keras_features ={}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in STYLE_LAYERS:\n",
    "    \n",
    "    # create a temporal subnetwork up to the actual layer \n",
    "    tmpmodel=Model(input=style_keras_model.input, output=style_keras_model.get_layer(layer).output) \n",
    "    \n",
    "    # calculate the values of the \"style features\" features on the style image\n",
    "    features=np.transpose(tmpmodel.predict(np.transpose(style_pre,(0,3,1,2)) ),(0,2,3,1))\n",
    "    print('features shape:', layer, features.shape)\n",
    "    \n",
    "    # rehape the features\n",
    "    features = np.reshape(features, (-1, features.shape[3]))\n",
    "    print('reshaped features shape:', layer, features.shape)\n",
    "    \n",
    "    # Create a covraince matrix (Gram matrix) from these features\n",
    "    gram = np.matmul(features.T, features) / features.size\n",
    "    style_keras_features[layer] = gram\n",
    "    print('gram shape:', layer, gram.shape)\n",
    "    print('**************')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new image we create, we want to match these style features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_features=style_keras_features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a new initial image. Random noise... or maybe load it from somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make stylized image using backpropogation\n",
    "initial = None\n",
    "#initial = scipy.misc.imread('./images/cat.jpg')\n",
    "if initial is None:\n",
    "    noise = np.random.normal(size=image_content.shape, scale=np.std(image_content) * 0.1)\n",
    "    initial = tf.random_normal(image_content.shape) * 0.256\n",
    "else:\n",
    "    initial = np.array([preprocess(initial, mean_pixel)])\n",
    "    initial = initial.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new image initialized with the above value. This will be the tensorflow variable that we will update with backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(initial)\n",
    "image_net = net(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the parameters. Feel free to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#content_weight = 5e0\n",
    "#style_weight = 1e4\n",
    "#tv_weight = 1e3\n",
    "#learning_rate = 1e0\n",
    "\n",
    "content_weight= 5e0\n",
    "style_weight= 1e2\n",
    "tv_weight = 1e2\n",
    "learning_rate = 1e1\n",
    "\n",
    "iterations =  1000\n",
    "checkpoint_iterations = 20\n",
    "print_iterations = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the content loss function. \n",
    "\n",
    "It measure how much the content features of the tensorflow variable \"image\" deviate from the content features of the content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content loss\n",
    "# content_loss = content_weight * (2 * tf.nn.l2_loss(\n",
    "#         image_net[CONTENT_LAYER] - content_features[CONTENT_LAYER]) / \n",
    "#         content_features[CONTENT_LAYER].size)\n",
    "content_loss = 0\n",
    "content_losses = []\n",
    "for content_layer in CONTENT_LAYERS:\n",
    "    content_losses.append(2 * tf.nn.l2_loss(\n",
    "                          image_net[content_layer] - content_features[content_layer]) / \n",
    "                          content_features[content_layer].size)\n",
    "content_loss += content_weight * reduce(tf.add, content_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the style loss function.\n",
    "It measure how much the style Gram matrices of the tensorflow variable \"image\" deviate from the style Gram matrices of the style image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# style loss\n",
    "style_loss = 0\n",
    "style_losses = []\n",
    "for style_layer in STYLE_LAYERS:\n",
    "    layer = image_net[style_layer]\n",
    "    _, height, width, number = map(lambda i: i.value, layer.get_shape())\n",
    "    size = height * width * number\n",
    "    feats = tf.reshape(layer, (-1, number))\n",
    "    gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "    style_gram = style_features[style_layer]\n",
    "    style_losses.append(2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "style_loss += style_weight * reduce(tf.add, style_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tensor_size(tensor):\n",
    "    from operator import mul\n",
    "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a Total Variation loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total variation denoising\n",
    "tv_y_size = _tensor_size(image[:,1:,:,:])\n",
    "tv_x_size = _tensor_size(image[:,:,1:,:])\n",
    "tv_loss = tv_weight * 2 * (\n",
    "        (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:image_content.shape[1]-1,:,:]) /\n",
    "            tv_y_size) +\n",
    "        (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:image_content.shape[2]-1,:]) /\n",
    "            tv_x_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss function is the sum of the content_loss, style_loss, and tv_loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# overall loss\n",
    "loss = content_loss + style_loss + tv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Adam to minimize the final loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer setup\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imsave(path, img):\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    scipy.misc.imsave(path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZATION STARTS NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization\n",
    "best_loss = float('inf')\n",
    "best = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer)\n",
    "    for i in range(iterations):\n",
    "        print(i)\n",
    "        \n",
    "        # the next step is the actual tensorflow training/minimization step\n",
    "        train_step.run()\n",
    "        \n",
    "        # save the images after some iterations\n",
    "        if i % checkpoint_iterations == 0 or i == iterations - 1:\n",
    "            this_loss = loss.eval()\n",
    "            if this_loss < best_loss:\n",
    "                best_loss = this_loss\n",
    "                \n",
    "                # this is the best image so far\n",
    "                best = image.eval()\n",
    "                print(\"new minimum found\")\n",
    "                newimg= unprocess(best.reshape(image_content.shape[1:]), mean_pixel)\n",
    "                img = np.clip(newimg, 0, 255).astype(np.uint8)\n",
    "                plt.figure(figsize=(11,11))\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "            \n",
    "            # save a check point\n",
    "            import os\n",
    "            try:\n",
    "                os.makedirs('./checks/'+str.split(content_file_name,'.')[0])\n",
    "            except OSError:\n",
    "                pass\n",
    "            timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename_cp = './checks/'+str.split(content_file_name,'.')[0]+'/'+timestr+'.jpg'\n",
    "            \n",
    "            cp = unprocess(best.reshape(image_content.shape[1:]), mean_pixel)\n",
    "            imsave(filename_cp, cp)\n",
    "           \n",
    "        \n",
    "        if i % print_iterations == 0 or i == iterations - 1:\n",
    "            print('Iteration %d/%d' % (i + 1, iterations))\n",
    "            print('  content loss: %g' % content_loss.eval())\n",
    "            print('    style loss: %g' % style_loss.eval())\n",
    "            print('       tv loss: %g' % tv_loss.eval())\n",
    "            print('    total loss: %g' % loss.eval())\n",
    "\n",
    "    output = unprocess(best.reshape(image_content.shape[1:]), mean_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imsave('output_'+content_file_name, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = 'output_'+content_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
